# import streamlit as st
# import os
# from langchain_groq import ChatGroq
# from langchain_huggingface import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.document_loaders import PyPDFLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.output_parsers import StrOutputParser

# # 1. INITIALISATION
# st.set_page_config(page_title="IA PDF Pro", layout="wide")

# if "chat_history" not in st.session_state:
#     st.session_state.chat_history = ""

# st.title("üõ°Ô∏è Assistant PDF Intelligent")

# # Sidebar
# st.sidebar.header("Configuration")
# groq_key = st.sidebar.text_input("Cl√© API Groq", type="password")
# if st.sidebar.button("üóëÔ∏è Effacer la m√©moire"):
#     st.session_state.chat_history = ""
#     st.rerun()

# uploaded_file = st.file_uploader("D√©posez votre PDF ici", type="pdf")

# # 2. TRAITEMENT DU PDF (Seulement si un fichier est pr√©sent)
# if uploaded_file and groq_key:
#     with open("temp.pdf", "wb") as f:
#         f.write(uploaded_file.getbuffer())
    
#     with st.spinner("Analyse du document..."):
#         loader = PyPDFLoader("temp.pdf")
#         docs = loader.load()
#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
#         chunks = text_splitter.split_documents(docs)
        
#         embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
#         vectorstore = FAISS.from_documents(chunks, embeddings)
#         retriever = vectorstore.as_retriever()

#         model = ChatGroq(groq_api_key=groq_key, model_name="llama-3.3-70b-versatile")
        
#         prompt = ChatPromptTemplate.from_template("""
#         R√©ponds en utilisant le contexte et l'historique fournis.
#         HISTORIQUE : {history}
#         CONTEXTE : {context}
#         QUESTION : {question}
#         R√âPONSE :
#         """)

#         def get_memory(_):
#             return st.session_state.get("chat_history", "")

#         chain = (
#             {
#                 "context": retriever, 
#                 "question": RunnablePassthrough(),
#                 "history": get_memory
#             }
#             | prompt
#             | model
#             | StrOutputParser()
#         )

#     st.success("‚úÖ Analyse termin√©e ! Posez votre question ci-dessous.")
    
#     # --- LA ZONE DE QUESTION (Bien visible ici) ---
#     # Ajoute une key unique pour que Streamlit ne se m√©lange pas les pinceaux
# # --- LA ZONE DE QUESTION (Bien align√©e √† l'int√©rieur du bloc 'if uploaded_file') ---
#     user_question = st.text_input(
#         "Votre question :", 
#         placeholder="Ex: De quoi parle ce document ?", 
#         key="user_input_field"
#     )
    
#     if user_question:
#         with st.spinner("L'IA r√©pond..."):
#             result = chain.invoke(user_question)
#             # Mise √† jour de la m√©moire
#             st.session_state.chat_history += f"\nUtilisateur: {user_question}\nAssistant: {result}\n"
#             st.info(result)

# # --- Ce bloc est align√© tout √† gauche avec le 'if uploaded_file' ---
# elif not groq_key:
#     st.info("üëã Entrez votre cl√© Groq dans la barre lat√©rale pour commencer.")
import streamlit as st
import os
import uuid
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- CONFIGURATION ---
st.set_page_config(page_title="PDF Intelligence Pro", layout="wide", page_icon="üí∞")

# Initialisation des variables de session
if "user_id" not in st.session_state:
    st.session_state.user_id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "question_count" not in st.session_state:
    st.session_state.question_count = 0

# --- PARAM√àTRES BUSINESS ---
LIMIT_GRATUITE = 3
# Remplace '#' par ton futur lien Stripe ou PayPal
LIEN_PAIEMENT = "https://paypal.me/votrecompte" 

st.title("üõ°Ô∏è PDF Intelligence Pro")

# --- SIDEBAR & BUSINESS LOGIC ---
with st.sidebar:
    st.header("üíé Espace Membre")
    groq_key = st.text_input("Cl√© API Groq", type="password", help="Entrez votre cl√© pour tester l'IA")
    
    st.divider()
    st.write(f"üìä Utilisation gratuite : **{st.session_state.question_count} / {LIMIT_GRATUITE}**")
    
    # Barre de progression visuelle
    progress = min(st.session_state.question_count / LIMIT_GRATUITE, 1.0)
    st.progress(progress)

    if st.session_state.question_count >= LIMIT_GRATUITE:
        st.error("üöÄ Limite gratuite atteinte !")
        st.markdown(f"""
            <a href="{LIEN_PAIEMENT}" target="_blank" style="text-decoration: none;">
                <div style="background-color: #00BA37; color: white; padding: 12px; border-radius: 8px; text-align: center; font-weight: bold; border: 2px solid #008f2a;">
                    üîì D√©bloquer l'Illimit√© (9,99‚Ç¨)
                </div>
            </a>
            <p style="font-size: 11px; color: gray; text-align: center; margin-top: 5px;">
                Acc√®s instantan√© apr√®s paiement
            </p>
        """, unsafe_allow_html=True)
    
    st.divider()
    if st.button("üßπ Nouvelle session"):
        st.session_state.messages = []
        st.session_state.vectorstore = None
        st.session_state.question_count = 0
        st.rerun()

# --- CHARGEMENT DU DOCUMENT ---
uploaded_file = st.file_uploader("√âtape 1 : D√©posez votre PDF", type="pdf")

if uploaded_file and groq_key:
    if st.session_state.vectorstore is None:
        unique_filename = f"temp_{st.session_state.user_id}.pdf"
        try:
            with open(unique_filename, "wb") as f:
                f.write(uploaded_file.getbuffer())
            with st.spinner("Analyse du document..."):
                loader = PyPDFLoader(unique_filename)
                chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(loader.load())
                embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
                st.session_state.vectorstore = FAISS.from_documents(chunks, embeddings)
                st.success("‚úÖ Document pr√™t !")
        finally:
            if os.path.exists(unique_filename): os.remove(unique_filename)

# --- INTERFACE DE CHAT ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt_input := st.chat_input("Posez votre question ici..."):
    # V√©rification des limites avant de r√©pondre
    if st.session_state.question_count >= LIMIT_GRATUITE:
        st.warning("‚ö†Ô∏è Limite atteinte. Veuillez utiliser le bouton dans la barre lat√©rale pour continuer.")
    elif not groq_key or st.session_state.vectorstore is None:
        st.info("Veuillez entrer votre cl√© API et charger un PDF.")
    else:
        # Affichage utilisateur
        st.session_state.messages.append({"role": "user", "content": prompt_input})
        with st.chat_message("user"):
            st.markdown(prompt_input)

        # R√©ponse Assistant
        with st.chat_message("assistant"):
            model = ChatGroq(groq_api_key=groq_key, model_name="llama-3.3-70b-versatile")
            history_text = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-5:]])
            
            qa_prompt = ChatPromptTemplate.from_template("""
            R√©ponds de fa√ßon pro. Contexte : {context}. Historique : {history}. Question : {question}
            """)

            chain = (
                {"context": st.session_state.vectorstore.as_retriever(), "question": RunnablePassthrough(), "history": lambda x: history_text}
                | qa_prompt | model | StrOutputParser()
            )
            
            response = chain.invoke(prompt_input)
            st.markdown(response)
            
            # Mise √† jour
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.session_state.question_count += 1
            st.rerun()