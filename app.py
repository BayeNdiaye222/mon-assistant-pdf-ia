# import streamlit as st
# import os
# from langchain_groq import ChatGroq
# from langchain_huggingface import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.document_loaders import PyPDFLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.runnables import RunnablePassthrough
# from langchain_core.output_parsers import StrOutputParser

# # 1. INITIALISATION
# st.set_page_config(page_title="IA PDF Pro", layout="wide")

# if "chat_history" not in st.session_state:
#     st.session_state.chat_history = ""

# st.title("üõ°Ô∏è Assistant PDF Intelligent")

# # Sidebar
# st.sidebar.header("Configuration")
# groq_key = st.sidebar.text_input("Cl√© API Groq", type="password")
# if st.sidebar.button("üóëÔ∏è Effacer la m√©moire"):
#     st.session_state.chat_history = ""
#     st.rerun()

# uploaded_file = st.file_uploader("D√©posez votre PDF ici", type="pdf")

# # 2. TRAITEMENT DU PDF (Seulement si un fichier est pr√©sent)
# if uploaded_file and groq_key:
#     with open("temp.pdf", "wb") as f:
#         f.write(uploaded_file.getbuffer())
    
#     with st.spinner("Analyse du document..."):
#         loader = PyPDFLoader("temp.pdf")
#         docs = loader.load()
#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
#         chunks = text_splitter.split_documents(docs)
        
#         embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
#         vectorstore = FAISS.from_documents(chunks, embeddings)
#         retriever = vectorstore.as_retriever()

#         model = ChatGroq(groq_api_key=groq_key, model_name="llama-3.3-70b-versatile")
        
#         prompt = ChatPromptTemplate.from_template("""
#         R√©ponds en utilisant le contexte et l'historique fournis.
#         HISTORIQUE : {history}
#         CONTEXTE : {context}
#         QUESTION : {question}
#         R√âPONSE :
#         """)

#         def get_memory(_):
#             return st.session_state.get("chat_history", "")

#         chain = (
#             {
#                 "context": retriever, 
#                 "question": RunnablePassthrough(),
#                 "history": get_memory
#             }
#             | prompt
#             | model
#             | StrOutputParser()
#         )

#     st.success("‚úÖ Analyse termin√©e ! Posez votre question ci-dessous.")
    
#     # --- LA ZONE DE QUESTION (Bien visible ici) ---
#     # Ajoute une key unique pour que Streamlit ne se m√©lange pas les pinceaux
# # --- LA ZONE DE QUESTION (Bien align√©e √† l'int√©rieur du bloc 'if uploaded_file') ---
#     user_question = st.text_input(
#         "Votre question :", 
#         placeholder="Ex: De quoi parle ce document ?", 
#         key="user_input_field"
#     )
    
#     if user_question:
#         with st.spinner("L'IA r√©pond..."):
#             result = chain.invoke(user_question)
#             # Mise √† jour de la m√©moire
#             st.session_state.chat_history += f"\nUtilisateur: {user_question}\nAssistant: {result}\n"
#             st.info(result)

# # --- Ce bloc est align√© tout √† gauche avec le 'if uploaded_file' ---
# elif not groq_key:
#     st.info("üëã Entrez votre cl√© Groq dans la barre lat√©rale pour commencer.")
import streamlit as st
import os
import uuid
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- CONFIGURATION DE LA PAGE ---
st.set_page_config(page_title="PDF Intelligence Pro", layout="wide", page_icon="üìÑ")

# R√©cup√©ration automatique de TA cl√© depuis les Secrets
# Plus besoin que l'utilisateur la saisisse !
GROQ_API_KEY = st.secrets["GROQ_API_KEY"]

# Initialisation des variables de session
if "user_id" not in st.session_state:
    st.session_state.user_id = str(uuid.uuid4())
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "question_count" not in st.session_state:
    st.session_state.question_count = 0

# --- PARAM√àTRES BUSINESS ---
LIMIT_GRATUITE = 3
VOTRE_NUMERO = "221760222771" # <--- METS TON NUM√âRO ICI
MESSAGE_WA = "Bonjour, je souhaite d√©bloquer l'acc√®s illimit√© (9,99‚Ç¨) √† l'Assistant PDF."
LIEN_WHATSAPP = f"https://wa.me/{VOTRE_NUMERO}?text={MESSAGE_WA.replace(' ', '%20')}"

st.title("üöÄ PDF Intelligence Pro")
st.caption("L'IA pour √©tudiants : Analysez vos cours en un clic.")

# --- SIDEBAR & MON√âTISATION ---
with st.sidebar:
    st.header("üíé Statut du compte")
    st.write(f"Questions : **{st.session_state.question_count} / {LIMIT_GRATUITE}**")
    
    progress = min(st.session_state.question_count / LIMIT_GRATUITE, 1.0)
    st.progress(progress)

    if st.session_state.question_count >= LIMIT_GRATUITE:
        st.error("üöÄ Limite gratuite atteinte !")
        st.markdown(f"""
            <a href="{LIEN_WHATSAPP}" target="_blank" style="text-decoration: none;">
                <div style="background-color: #25D366; color: white; padding: 15px; border-radius: 10px; text-align: center; font-weight: bold;">
                    üí¨ D√©bloquer l'Illimit√© via WhatsApp
                </div>
            </a>
        """, unsafe_allow_html=True)
    
    st.divider()
    if st.button("üßπ Nouvelle session"):
        st.session_state.messages = []
        st.session_state.vectorstore = None
        st.session_state.question_count = 0
        st.rerun()

# --- CHARGEMENT DU PDF ---
uploaded_file = st.file_uploader("Chargez votre PDF pour commencer", type="pdf")

if uploaded_file:
    if st.session_state.vectorstore is None:
        unique_filename = f"temp_{st.session_state.user_id}.pdf"
        try:
            with open(unique_filename, "wb") as f:
                f.write(uploaded_file.getbuffer())
            with st.spinner("L'IA analyse votre document..."):
                loader = PyPDFLoader(unique_filename)
                chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(loader.load())
                embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
                st.session_state.vectorstore = FAISS.from_documents(chunks, embeddings)
                st.success("‚úÖ Pr√™t ! Posez votre question.")
        finally:
            if os.path.exists(unique_filename): os.remove(unique_filename)

# --- ZONE DE CHAT ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt_input := st.chat_input("Votre question..."):
    if st.session_state.question_count >= LIMIT_GRATUITE:
        st.warning("‚ö†Ô∏è Limite atteinte. Contactez-nous sur WhatsApp pour continuer.")
    elif st.session_state.vectorstore is None:
        st.info("Veuillez d'abord charger un PDF.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt_input})
        with st.chat_message("user"):
            st.markdown(prompt_input)

        with st.chat_message("assistant"):
            # Ici on utilise la cl√© secr√®te directement
            model = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama-3.3-70b-versatile")
            
            history_text = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-5:]])
            qa_prompt = ChatPromptTemplate.from_template("Contexte: {context}\nHistorique: {history}\nQuestion: {question}")

            chain = (
                {"context": st.session_state.vectorstore.as_retriever(), "question": RunnablePassthrough(), "history": lambda x: history_text}
                | qa_prompt | model | StrOutputParser()
            )
            
            response = chain.invoke(prompt_input)
            st.markdown(response)
            
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.session_state.question_count += 1
            st.rerun()